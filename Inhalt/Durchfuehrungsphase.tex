% !TEX root = ../Projektdokumentation.tex
\section{Durchführungsphase}
\label{sec:Durchführungsphase}

\subsection{Vorbereitung der Tests}
\label{sec:VorbereitungTests}

\subsubsection{Vorbereiten der Hardware}
\label{sec:VorbereitungHardware}
Eine ausrangierte DELL Precision Workstation dient als Hardwareplattform für das Projekt. Die Ausstattung von 32 Gigabyte Arbeitsspeicher und einem Intel Xeon E5-1650 v2 Hochleistungsprozessor ist ausreichend für den Betrieb von mehreren virtuellen Maschinen.

Um den Bedingungen in einem \glqq{}echten\grqq{} Rechencenter möglichst nahe zu kommen, wird der VM-Hypervisor mit zwei Netzwerkschnittstellen ausgestattet. Ein Interface ist für die Kommunikation mit dem Hypervisor selbst vorgesehen (sogenanntes Management-LAN), und eine weitere Netzwerkschnittstelle wird von den virtuellen Maschinen benutzt, um im Netzwerk erreichbar zu sein. Da die Workstation nur über einen Netzwekport verfügte, musste eine zweite Netzwerkkarte eingebaut werden.

\subsubsection{Installation des Hypervisors}
\label{sec:InstallationHypervisor}
Um die gegeben Ressourcen optimal auszunutzen, wird ein sogenannter Typ 1-Hypervisor eingesetzt. Dieser kommuniziert direkt mit der Hardware, ohne dass dazwischen ein anderes Betriebssystem zum Einsatz kommt. Für dieses Projekt wird das System \glqq{}ESXi\grqq{} von VMware in der Version 6.7 verwendet, das auch in den firmeneigenen Rechenzentren zum Einsatz kommt.

Die Installation gestaltet sich als simpel. Nachdem wichtige UEFI-Optionen angepasst wurden (UEFI statt Legacy-BIOS; hardwareseitige Virtualisierungsunterstützung) wird das System von einem USB-Datenträger, der zuvor mit dem ESXi-Image geflasht wurde, gestartet. Nachdem die für die Installation zu benutzende Festplatte ausgewählt, und ein Root-Passwort vergeben wurde, startet der Installationsvorgang. Nach abgeschlossener Installation muss noch eine IP-Adresse für das Managementnetzwerk vergeben werden.

Die restliche Konfiguration geschieht bequem über eine Weboberfläche. Der Lizenzschlüssel wird hinterlegt, die zweite Festplatte als Datastore eingebunden, und das Netzwerk für die virtuellen Maschinen konfiguriert (siehe Abbildung \ref{screen:vmnetwork}). Hierzu wird ein neuer virtueller Switch für den zweiten Netzwerkport (der, der nicht mit dem Management-LAN belegt ist) erstellt und mit einer neuen Port Group versehen. Diese Port Groups werden in ESXi verwendet, um logische Netzwerkschnittstellen bereit zu stellen und zu verwalten. 

\subsubsection{Installation der virtuellen Maschinen}
\label{sec:InstallationVMs}
Für virtuelle Maschinen muss in der ESXi-Weboberfläche zunächst die Systemkonfiguration festgelegt werden. Hierfür werden die für die VM vorgesehene CPU-Kerne, Arbeits- und Massenspeicherspeicherkapazität (siehe Abbildung \ref{screen:vmcreation}) sowie die zu benutzenden Port Groups eingestellt. Abschließend wird in ein virtuelles DVD-Laufwerk das ISO-Abbild für das entsprechende zu installierende Betriebssystem eingehängt, und die virtuelle Maschine gestartet. Die anschließende Betriebssysteminstallation kann über eine emulierte Browser-Konsole durchgeführt werden.

Um in diesem Testsystem möglichst nah an das Firmennetzwerk heranzukommen, wurden drei Betriebssysteme für die Server ausgewählt: Windows Server 2019, Ubuntu 18.04 LTS sowie Debian 10.3. Die Installation der Systeme verlief ohne Probleme.

\subsubsection{Installation von \glqq{}Icinga\grqq{}}
\label{sec:InstallationIcinga}
Die Installation von \glqq{}Icinga\grqq{} unter Ubuntu beginnt mit dem Aufruf der Rootshell und einem anschließendem kompletten Systemupdate. Die Pakete \code{icinga2} und \code{icingacli} sind für die Monitoring-Engine an sich, sowie die Verwaltung über Kommandozeile zuständig und können über die Paketverwaltung (hier: apt) installiert werden. Das Paket \code{monitoring-plugins} beinhaltet Plugins für die wichtigsten Monitoringaufgaben (z.B. Ping).
\footnote{Eine komplette (nachträglich optimierte) Liste an Befehlen, die für eine komplette Installation unter Ubuntu 18.04 ausgeführt werden muss, findet sich im \Anhang{app:skript}}

Aufgrund der firmeninteren Verbreitung, sowie der In-Memory Unterstützung, wird MySQL als Datenbanksystem verwendet. Durch Installation der Pakete \code{mysql-server} und \code{mysql-client} werden ein MySQL-Server sowie ein MySQL-Client bereitgestellt, das Skript \code{mysql\_{}secure\_{}installation} (siehe Abbildung \ref{screen:mysqlsecure}) verbessert die Sicherheit durch Maßnahmen wie das Entfernen von anonymen Accounts oder die Absicherung des root-Accounts.

Im nächsten Schritt muss die Schnittstelle zwischen \glqq{}Icinga Data Output\grqq{} (Exportfunktion für Monitoringdaten; kurz IDO) und MySQL geschaffen werden, damit die gesammelten Daten auch gespeichert werden können. Dies geschieht durch Installation des Pakets \code{icinga2-ido-mysql}. Anschließend kann das Webinterface \code{icingaweb2} installiert werden.

Beim Versuch, das installierte Web-Frontend aufzurufen, wurde lediglich ein PHP-Fehler angezeigt. Es ergab sich, dass dieser Fehler durch Inkompatibilitäten mit bestimmten PHP 7.2 Plugins verursacht wird. Dieser Fehler wurde bereits Mitte 2018 behoben, allerdings ist der Bugfix anscheinend nicht in das Paket eingespielt worden. Das Problem wurde gelöst, indem die aktuelle Version von \glqq{}Icinga Web 2\grqq{} vom öffentlichen Github-Repository nach \code{/usr/share/icingaweb2} geklont wurde.

Ist \glqq{}Icinga Web 2\grqq{} fertig installiert, muss noch ein Benutzer für die MySQL-Datenbank erstellt werden. Abschließend wird IDO mittels \code{icinga2 feature enable command ido-mysql} aktiviert, und ein Setup-Token mit \code{icingacli setup token create} generiert. Nach einem Neustart des icinga2-Dienstes (\code{systemctl restart icinga2}) ist \glqq{}Icinga 2\grqq{} bereit, eingerichtet zu werden.

\paragraph{Einrichtung über Webfrontend}
Im Browser wird nun (IP-Adresse des Servers)/icingaweb2/setup aufgerufen, um den Konfigurationsassistenten (siehe Abbildung \ref{screen:konfigassistent}) zu starten. Nach Eingabe des vorher generierten Setup-Tokens wird zunächst geprüft, ob alle notwendigen PHP-Plugins vorhanden sind. In der für diese Projekt durchgeführten Installation fehlte das Plugin \glqq{}PDO-PostgreSQL\grqq{}, welches aufgrund der Verwendung von MySQL nicht benötigt wird, sowie \glqq{}cURL\grqq{}, das mit der Paketverwaltung nachinstalliert wurde.

Im Abschnitt \glqq{}Configuration\grqq{} werden noch kleinere, weitere Einstellungen getroffen, hierbei bietet sich aber an, die Standardeinstellungen beizubehalten. Eine Ausnahme stellt die bevorzugte Authentifizierungsmethode für Nutzende des Frontends dar; hier kann zwischen LDAP und einer MySQL-Datenbank gewählt werden. Für dieses Projekt wurde aufgrund des geringeren Umfangs letztere Option gewählt, die Konfiguration hierfür findet sich in Abbildung \ref{screen:userdb}.

\paragraph{Einpflegen von Servern}
Durch Installation des Plugins \glqq{}Icinga Director\grqq{} kann das Hinzufügen von zu überwachenden Servern über die Webschnittstelle erledigt werden; für dieses Projekt wird darauf verzichtet und die weitergehende Konfiguration geschieht über Anpassung der Konfigurationsdateien unter \code{/etc/icinga2/conf.d/}. Dort können in der Datei \code{hosts.conf} neue Server hinzugefügt werden (siehe Abbildung \ref{screen:newservers}). Abschließend kann die Syntax der Konfigurationsdateien mittels \code{icinga2 daemon -C} überprüft werden; nach einem Neustart des \glqq{}Icinga 2\grqq{} Dienstes sind die neuen Hosts im Monitoring verfügbar.

\paragraph{Installation des Agents}
Mit der bisherigen Konfiguration können einfache Checks (z.B. Ping) auf die entsprechenden Hosts durchgeführt werden. Für aufwendigere Überprüfungen wie Speicherplatzkontrolle muss auf den Hosts eine Software installiert werden. Hierfür muss zunächst der Monitoringserver als \glqq{}Master\grqq{} deklariert werden, dies geschieht mittels des Befehls \code{icinga2 node wizard}. Anschließend wird mit \code{sudo icinga2 pki ticket --cn (FQDN)} noch ein \glqq{}Ticket\grqq{} für den angegebenen FQDN erstellt, mithilfe dessen der Host die Verbindung zum Master herstellen kann.

Der Host selbst installiert die Software ebenfalls, auf Linux-basierten Systemen ist diese im Paket \code{icinga2} enthalten, für Windows existieren herunterladbare Installationsdateien. Mit Ausführung des Befehls \code{icinga2 node wizard} wird die Konfiguration des Agents abgeschlossen, bei Windows geschieht dies über ein grafisches Userinterface. Nachdem die Adresse des Masters und die eben erstellte Ticketnummer angegeben wurden, stellt der Agent eine Verbindung zum Monitoring her.

\subsection{Durchführung der Tests}
\label{sec:DurchführungTests}

\subsubsection{Überwachung von Leistungsparametern}
\label{sec:ÜberwachungLeistungsparameter}
Das installierte Pluginpaket \code{monitoring-plugins} beinhaltet Plugins zur Überwachung der Leistungsparametern (CPU-Auslastung, RAM-Belegung und belegter Massenspeicher). Für diese Überprüfungen müssen in \code{/etc/icinga2/conf.d/commands.conf} Definitionen erstellt werden. Am Beispiel in Abbildung \ref{screen:loaddefinition} ist erkenntlich, dass diese Definitionen neben dem auszuführendem Befehl (diese sind als Bash-Skript abgelegt; das Verzeichnis hierfür ist in der Konstante \glqq{}PluginDir\grqq{} gespeichert) auch die zu übergebenden Argumente enthält. Im Beispiel der CPU-Auslastung wird mitgegeben, dass der Durchschnitt aller CPU-Kerne berechnet (\code{-r}) und bei bestimmten Schwellenwerten (\code{-{}-critical}) ein Alarm gegeben werden soll.

Ist die Definition erfolgreich validiert, muss sie noch den entsprechenden Hosts zugewiesen werden. In der Datei \code{/etc/icinga2/conf.d/services.conf} wird hierzu ein Eintrag, wie in Abbildung \ref{screen:service} gezeigt, erstellt; dieser gibt den entsprechenden Befehl (\code{check\_{}command}) und die zu überprüfende Instanz (\code{command\_{}endpoint}) an, und weist den Befehl zum Schluss allen Hosts mit einer Adresse zu (\code{assign where host.address}).

Die Konfiguration der anderen Checks (RAM-Auslastung und Massenspeicherbelegung) läuft gleichermaßen. Die Überwachung der nach dem Kriterienkatalog benötigten Parameter funktioniert problemlos und die Werte decken sich mit den von den Systemen ermittelten Auslastungen. Somit ist \glqq{}Icinga 2\grqq{} hierfür einsatzfähig.

\subsubsection{Betriebssystemkompatibilität}
\label{sec:oscompatibility}
Es konnten keine Probleme bei der Integration von anderen Betriebsystemen festgestellt werden. Sowohl unter Ubuntu 18.04, Debian 10 und Windows Server 1809 läuft \glqq{}Icinga 2\grqq{} ohne Probleme. Eine umfassende Betriebssystemkompatibilität ist somit gewährleistet.

\subsubsection{Webserverüberwachung}
\label{sec:ÜberwachungWebserver}
Die Überwachung von Webservern muss analog zu den vorhergehenden Checks eingerichtet werden. Das Plugin \glqq{}http\grqq{} übernimmt diese Aufgabe.  In der Abbildung \ref{screen:webserver} ist gut erkenntlich, wie die Antwort der HTTP-Abfrage (Statuscode 200 OK) ausgewertet und dargestellt wird. Für die Antwortzeit lassen sich auch hier Schwellenwerte konfigurieren, sodass nicht nur Erreichbarkeit, sondern auch Performance überprüft werden kann. HTTPS-Abfragen werden durch ein gleichnamiges Plugin ebenfalls unterstützt. Die Überwachung von Webdiensten ist somit nach den in der Planungsphase aufgestellten Kriterien möglich.

\subsubsection{Grafische Aufbereitung}
\label{sec:GrafischeAufbereitung}
Auf der Startseite lassen sich mehrere Dashboards erstellen, die mithilfe sogenannter \glqq{}Dashlets\grqq{} (URL jedes aufrubaren Menüs) befüllt werden können. Somit ist es einfach, sehr angepasste  und frei konfigurierbare Übersichten zu erstellen (siehe Abbildung \ref{screen:dashboard}).

Sollte dies nicht ausreichen, bietet die Entwicklercommunity viele weitere Plugins für diesen Zweck an. \code{dashin-dashboard} wird schon am Standort Bremen eingesetzt. Nach Auskunft der Mitarbeitenden ist dieses in der Lage, noch ansprechendere Dashboards zu erstellen. Auch hier konnten somit die vorher definierten Kriterien erfüllt werden.